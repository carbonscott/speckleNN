{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0b1d4f-1984-4fe4-9433-9a1dbd48732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:PROJ_ROOT) -> /reg/data/ana03/scratch/cwang31/spi\n",
      "/reg/data/ana03/scratch/cwang31/spi\n"
     ]
    }
   ],
   "source": [
    "# Run the notebook as if it's in the PROJECT directory\n",
    "%bookmark PROJ_ROOT /reg/data/ana03/scratch/cwang31/spi\n",
    "%cd -b PROJ_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42bb13ba-ed26-4c91-9827-13a97cd05d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2713089a-2b3d-4eb9-b239-453b7ace4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_fonts():\n",
    "    # Where to load external font...\n",
    "    drc_py    = '.'\n",
    "    drc_font  = os.path.join(\"fonts\", \"Helvetica\")\n",
    "    fl_ttf    = f\"Helvetica.ttf\"\n",
    "    path_font = os.path.join(drc_py, drc_font, fl_ttf)\n",
    "    prop_font = font_manager.FontProperties( fname = path_font )\n",
    "\n",
    "    # Add Font and configure font properties\n",
    "    font_manager.fontManager.addfont(path_font)\n",
    "    prop_font = font_manager.FontProperties(fname = path_font)\n",
    "\n",
    "    # Specify fonts for pyplot...\n",
    "    plt.rcParams['font.family'] = prop_font.get_name()\n",
    "    plt.rcParams['font.size']   = 14\n",
    "\n",
    "    return None\n",
    "\n",
    "config_fonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5314621f-b5a1-410d-a0c3-eed0535ffd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "drc_pickle = \"confusion_matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69719066-2b6c-459d-834b-cfd6ffa807bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2023 16:32:54 INFO image_preprocess_faulty_sq_for_validate - ___/ Preprocess Settings \\___\n",
      "01/20/2023 16:32:54 INFO image_preprocess_faulty_sq_for_validate - Apply Poisson noise. \n",
      "01/20/2023 16:32:54 INFO image_preprocess_faulty_sq_for_validate - Apply Gaussian noise. sigma = 0.15.\n",
      "01/20/2023 16:32:54 INFO image_preprocess_faulty_sq_for_validate - TRANS : Apply cropping.\n",
      "01/20/2023 16:32:54 INFO image_preprocess_faulty_sq_for_validate - TRANS : Apply random rotation. angle = None, center = (48, 48).\n",
      "01/20/2023 16:32:54 INFO deepprojection.encoders.convnet     - ___/ Configure Encoder \\___\n",
      "01/20/2023 16:32:54 INFO deepprojection.encoders.convnet     - KV - dim_emb          : 128\n",
      "01/20/2023 16:32:54 INFO deepprojection.encoders.convnet     - KV - size_y           : 96\n",
      "01/20/2023 16:32:54 INFO deepprojection.encoders.convnet     - KV - size_x           : 96\n",
      "01/20/2023 16:32:54 INFO deepprojection.encoders.convnet     - KV - isbias           : True\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - ___/ NEURAL NETWORK SHAPE \\___\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - layer: 0, <class 'torch.nn.modules.conv.Conv2d'>\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - in : 1, 96, 96\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - out: 32, 92, 92\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - \n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - layer: 3, <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - in : 32, 92, 92\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - out: 32, 46, 46\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - \n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - layer: 4, <class 'torch.nn.modules.conv.Conv2d'>\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - in : 32, 46, 46\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - out: 64, 42, 42\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - \n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - layer: 7, <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - in : 64, 42, 42\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - out: 64, 21, 21\n",
      "01/20/2023 16:32:54 INFO deepprojection.utils                - \n",
      "01/20/2023 16:32:54 INFO deepprojection.model                - ___/ Configure Siamese Model \\___\n",
      "01/20/2023 16:32:54 INFO deepprojection.model                - KV - alpha            : 0.02\n",
      "01/20/2023 16:32:54 INFO deepprojection.model                - KV - encoder          : FewShotModel(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): PReLU(num_parameters=1)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (embed): Sequential(\n",
      "    (0): Linear(in_features=28224, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 6N38, 2.0, 0.981...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import socket\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from deepprojection.datasets.lite    import SPIOnlineDataset\n",
    "from deepprojection.model            import OnlineTripletSiameseModel, ConfigSiameseModel\n",
    "from deepprojection.encoders.convnet import FewShotModel, ConfigEncoder\n",
    "from deepprojection.utils            import EpochManager, MetaLog, init_logger, split_dataset, set_seed, ConfusionMatrix\n",
    "\n",
    "from image_preprocess_faulty_sq_for_validate import DatasetPreprocess\n",
    "\n",
    "\n",
    "# [[[ SEED ]]]\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "# [[[ CONFIG ]]]\n",
    "# [USER] Choose a model \n",
    "timestamp       = \"2023_0101_0856_44\"\n",
    "epoch           = 71\n",
    "num_max_support = 5\n",
    "# num_max_support = 20\n",
    "tag             = f\"seed_{seed}.support_{num_max_support}\"\n",
    "\n",
    "# [USER] Select a pdb\n",
    "pdb = '6N38'\n",
    "# pdb = '4B2Q'\n",
    "# pdb = '6YN1'\n",
    "# pdb = '6WM3'\n",
    "# pdb = '3DG5'\n",
    "\n",
    "# [USER] Choose a scaling exponent\n",
    "# scaling_exponent = np.linspace(-2, 2, 100)[100 // 2 - 1]\n",
    "# scaling_exponent = 0.0\n",
    "scaling_exponent = 2.0\n",
    "\n",
    "\n",
    "fl_chkpt = f\"{timestamp}.epoch={epoch}.chkpt\"\n",
    "\n",
    "# Define the test set\n",
    "# [USER]\n",
    "size_sample_query = 1000\n",
    "frac_support      = 0.4\n",
    "size_batch        = 100\n",
    "trans             = None\n",
    "alpha             = 0.02\n",
    "\n",
    "# Initialize a log file...\n",
    "init_logger(log_name = 'validate.query.test', timestamp = timestamp, returns_timestamp = False, saves_log = False)\n",
    "\n",
    "\n",
    "# Set up the right device for the computation...\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# [[[ DATASET ]]]\n",
    "# Set up parameters for an experiment...\n",
    "drc_dataset  = 'fastdata.h5/'\n",
    "fl_dataset   = f'{pdb}.relabel.pickle'\n",
    "path_dataset = os.path.join(drc_dataset, fl_dataset)\n",
    "\n",
    "# Load raw data...\n",
    "with open(path_dataset, 'rb') as fh:\n",
    "    dataset_list = pickle.load(fh)\n",
    "\n",
    "\n",
    "photon_scale = 10 ** scaling_exponent\n",
    "\n",
    "# Increase the photon intensities...\n",
    "dataset_rescale_list = [ (img * photon_scale, label, metadata) for (img, label, metadata) in dataset_list ]\n",
    "\n",
    "# Split data into two -- support set and query set...\n",
    "data_support, data_query = split_dataset(dataset_rescale_list, frac_support, seed = None)\n",
    "\n",
    "# Fetch all hit labels...\n",
    "hit_list = list(set( [ hit for _, (pdb, hit), _ in data_support ] ))\n",
    "\n",
    "# Form support set...\n",
    "support_hit_to_idx_dict = { hit : [] for hit in hit_list }\n",
    "for enum_data, (img, label, metadata) in enumerate(data_support):\n",
    "    _, hit = label\n",
    "    support_hit_to_idx_dict[hit].append(enum_data)\n",
    "\n",
    "for hit, idx_support in support_hit_to_idx_dict.items():\n",
    "    if len(support_hit_to_idx_dict[hit]) > num_max_support:\n",
    "        support_hit_to_idx_dict[hit] = random.sample(support_hit_to_idx_dict[hit], k = num_max_support)\n",
    "\n",
    "# Form query dataset...\n",
    "dataset_query = SPIOnlineDataset( dataset_list   = data_query, \n",
    "                                  prints_cache_state = False,\n",
    "                                  size_sample    = size_sample_query,\n",
    "                                  joins_metadata = False,\n",
    "                                  trans          = trans, )\n",
    "\n",
    "\n",
    "# [[[ Preprocess dataset ]]]\n",
    "# Data preprocessing can be lengthy and defined in dataset_preprocess.py\n",
    "img_orig            = dataset_query[0][0][0]   # idx, fetch img\n",
    "dataset_preproc     = DatasetPreprocess(img_orig)\n",
    "trans               = dataset_preproc.config_trans()\n",
    "dataset_query.trans = trans\n",
    "img_trans           = dataset_query[0][0][0]\n",
    "\n",
    "# [[[ IMAGE ENCODER ]]]\n",
    "# Config the encoder...\n",
    "dim_emb        = 128\n",
    "size_y, size_x = img_trans.shape[-2:]\n",
    "config_encoder = ConfigEncoder( dim_emb = dim_emb,\n",
    "                                size_y  = size_y,\n",
    "                                size_x  = size_x,\n",
    "                                isbias  = True )\n",
    "encoder = FewShotModel(config_encoder)\n",
    "\n",
    "# [[[ DEVICE ]]]\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# [[[ MODEL ]]]\n",
    "# Config the model...\n",
    "config_siamese = ConfigSiameseModel( alpha = alpha, encoder = encoder, )\n",
    "model = OnlineTripletSiameseModel(config_siamese)\n",
    "model.init_params(fl_chkpt = fl_chkpt)\n",
    "model.to(device = device)\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "dataset_query.cache_dataset()\n",
    "\n",
    "# [[[ EMBEDDING (SUPPORT) ]]]\n",
    "support_batch_emb_dict = { hit : None for hit in hit_list }\n",
    "for hit in hit_list:\n",
    "    support_idx_list    = support_hit_to_idx_dict[hit]\n",
    "    num_example_support = len(support_idx_list)\n",
    "    for enum_support_idx, support_idx in enumerate(support_idx_list):\n",
    "        # Fetch data from support...\n",
    "        img = data_support[support_idx][0]\n",
    "        img = trans(img)\n",
    "        img = img[None,]\n",
    "\n",
    "        # Normalize the image...\n",
    "        img = (img - img.mean()) / img.std()\n",
    "\n",
    "        # Preallocate tensor...\n",
    "        if enum_support_idx == 0:\n",
    "            size_c, size_y, size_x = img.shape\n",
    "            batch_img = torch.zeros((num_example_support, size_c, size_y, size_x))\n",
    "\n",
    "        # Save image as tensor...\n",
    "        batch_img[enum_support_idx] = torch.tensor(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_img = batch_img.to(device = device)\n",
    "        support_batch_emb_dict[hit] = model.encoder.encode(batch_img)\n",
    "\n",
    "# [[[ EMBEDDING (QUERY) ]]]\n",
    "num_test       = len(dataset_query)\n",
    "query_idx_list = range(num_test)\n",
    "for enum_query_idx, i in enumerate(query_idx_list):\n",
    "    # Fetch data from query\n",
    "    img = dataset_query[i][0]\n",
    "\n",
    "    # Preallocate tensor...\n",
    "    if enum_query_idx == 0:\n",
    "        size_c, size_y, size_x = img.shape\n",
    "        batch_img = torch.zeros((num_test, size_c, size_y, size_x))\n",
    "\n",
    "    # Save image as tensor...\n",
    "    batch_img[enum_query_idx] = torch.tensor(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_img = batch_img.to(device = device)\n",
    "    query_batch_emb = model.encoder.encode(batch_img)\n",
    "\n",
    "\n",
    "# [[[ METRIC ]]]\n",
    "diff_query_support_dict = {}\n",
    "for hit in hit_list:\n",
    "    # Q: number of query examples.\n",
    "    # S: number of support examples\n",
    "    # E: dimension of an embedding\n",
    "    # diff_query_support_dict[hit]: Q x S x E\n",
    "    # query_batch_emb[:, None]    : Q x 1 x E\n",
    "    # support_batch_emb_dict[hit] :     S x E\n",
    "    diff_query_support_dict[hit] = query_batch_emb[:,None] - support_batch_emb_dict[hit]\n",
    "\n",
    "hit_to_dist_dict = {}\n",
    "for hit in hit_list:\n",
    "    # Q: number of query examples.\n",
    "    # S: number of support examples\n",
    "    # hit_to_dist_dict[hit]: Q x S\n",
    "    hit_to_dist_dict[hit] = torch.sum(diff_query_support_dict[hit] * diff_query_support_dict[hit], dim = -1)\n",
    "\n",
    "# Use enumeration as an intermediate to obtain predicted hits...\n",
    "enum_to_hit_dict = {}\n",
    "\n",
    "# Encode hit type with enum\n",
    "# enum 0 : hit 1\n",
    "# enum 1 : hit 2\n",
    "for enum_hit, hit in enumerate(hit_list):\n",
    "    enum_to_hit_dict[enum_hit] = hit\n",
    "\n",
    "    # Fetch the values and indices of the closet support for this hit type for the query...\n",
    "    mean_support_val = hit_to_dist_dict[hit].mean(dim = -1)\n",
    "    if enum_hit == 0:\n",
    "        # H: number of hit types (single vs multi)\n",
    "        # N: number of examples\n",
    "        # mean_support_tensor: H x N\n",
    "        mean_support_tensor = torch.zeros((len(hit_list), *mean_support_val.shape))\n",
    "\n",
    "    mean_support_tensor[enum_hit] = mean_support_val\n",
    "\n",
    "# Obtain the predicted hit...\n",
    "# Obtain the min among examples across all hit type (dim = 0)\n",
    "pred_hit_as_enum_list = mean_support_tensor.min(dim = 0)[1]\n",
    "\n",
    "# Obtain the predicted hit for each input example\n",
    "pred_hit_list = [ enum_to_hit_dict[enum.item()] for enum in pred_hit_as_enum_list ]\n",
    "\n",
    "# Obtain the real hit...\n",
    "real_hit_list = [ dataset_query[idx][1][1] for idx in query_idx_list ]\n",
    "\n",
    "# New container to store validation result (thus res_dict) for each label...\n",
    "res_dict = {}\n",
    "for hit in hit_list: res_dict[hit] = { i : [] for i in hit_list }\n",
    "\n",
    "for pred_hit, real_hit in zip(pred_hit_list, real_hit_list):\n",
    "    res_dict[pred_hit][real_hit].append( None )\n",
    "\n",
    "cm = ConfusionMatrix(res_dict)\n",
    "acc = cm.get_metrics(1)[0]\n",
    "print(f\"Working on {pdb}, {scaling_exponent}, {acc}...\")\n",
    "\n",
    "\n",
    "# [USER INPUT]\n",
    "for k in hit_to_dist_dict.keys():\n",
    "    hit_to_dist_dict[k] = hit_to_dist_dict[k].cpu()\n",
    "fl_hit_to_dist_dict = f'hit_to_dist_dict.{pdb}.{timestamp}.epoch_{epoch}.{tag}.scale_{int(photon_scale)}.mean_dist.pickle'\n",
    "path_hit_to_dist_dict = os.path.join(drc_pickle, fl_hit_to_dist_dict)\n",
    "with open(path_hit_to_dist_dict, 'wb') as handle:\n",
    "    pickle.dump(hit_to_dist_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b8156-721a-4cde-b8c9-3732bab85440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana-ml-py3",
   "language": "python",
   "name": "ana-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

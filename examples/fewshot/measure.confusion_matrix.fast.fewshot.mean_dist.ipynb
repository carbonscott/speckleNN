{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2c5fc-cf65-40d4-9124-e15e2230374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the notebook as if it's in the PROJECT directory\n",
    "%bookmark PROJ_ROOT /reg/data/ana03/scratch/cwang31/spi\n",
    "%cd -b PROJ_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282fd87-d1f4-441b-aba7-7b23876f33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bd451-551a-4c0b-8d2b-fc00f31ad732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths for using psana\n",
    "%env SIT_ROOT=/reg/g/psdm/\n",
    "%env SIT_DATA=/cds/group/psdm/data/\n",
    "%env SIT_PSDM_DATA=/cds/data/psdm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb21255-acc4-467d-b1e3-7e5fb81be49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74312b-7770-4c6e-b903-c65d8cef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import socket\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from speckleNN.datasets.lite    import SPIDataset         , SPIOnlineDataset, MultiwayQueryset\n",
    "from speckleNN.model            import SiameseModelCompare, ConfigSiameseModel\n",
    "from speckleNN.validator        import MultiwayQueryValidator, ConfigValidator\n",
    "from speckleNN.encoders.convnet import Hirotaka0122       , ConfigEncoder\n",
    "from speckleNN.utils            import EpochManager       , MetaLog, init_logger, split_dataset, set_seed, ConfusionMatrix\n",
    "from datetime import datetime\n",
    "from image_preprocess import DatasetPreprocess\n",
    "# from image_preprocess_faulty import DatasetPreprocess\n",
    "# from image_preprocess_half import DatasetPreprocess\n",
    "# from image_preprocess_one_four import DatasetPreprocess\n",
    "# from image_preprocess_three_four import DatasetPreprocess\n",
    "\n",
    "\n",
    "# [[[ SEED ]]]\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "\n",
    "# [[[ CONFIG ]]]\n",
    "# timestamp = \"2022_1203_1807_16\"\n",
    "timestamp = \"2022_1130_2316_55\"\n",
    "frac_train = 0.5\n",
    "frac_validate = 0.5\n",
    "num_max_support = 5\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "size_sample_test = 1000\n",
    "size_sample_per_class = None\n",
    "size_batch = 100\n",
    "online_shuffle = True\n",
    "trans = None\n",
    "\n",
    "\n",
    "# Configure the location to run the job...## \n",
    "drc_cwd = os.getcwd()\n",
    "\n",
    "init_logger(log_name = 'validate.query.test', timestamp = timestamp, returns_timestamp = False)\n",
    "\n",
    "\n",
    "# Clarify the purpose of this experiment...\n",
    "hostname = socket.gethostname()\n",
    "comments = f\"\"\"\n",
    "            Hostname: {hostname}.\n",
    "\n",
    "            Online training.\n",
    "\n",
    "            Sample size (test)     : {size_sample_test}\n",
    "            Sample size (per class) : {size_sample_per_class}\n",
    "            Batch  size             : {size_batch}\n",
    "            Online shuffle          : {online_shuffle}\n",
    "            lr                      : {lr}\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "# [[[ DATASET ]]]\n",
    "# Set up parameters for an experiment...\n",
    "drc_dataset   = 'fastdata'\n",
    "fl_dataset    = '0000.fastdata'    # Raw, just give it a try\n",
    "path_dataset  = os.path.join(drc_dataset, fl_dataset)\n",
    "\n",
    "# Load raw data...\n",
    "with open(path_dataset, 'rb') as fh:\n",
    "    dataset_list = pickle.load(fh)\n",
    "\n",
    "# Split data...\n",
    "data_train   , data_val_and_test = split_dataset(dataset_list     , frac_train   , seed = None)\n",
    "data_validate, data_test         = split_dataset(data_val_and_test, frac_validate, seed = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0229011-665e-4f09-9428-801fd0a6b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test set\n",
    "dataset_query = SPIOnlineDataset( dataset_list = data_test, \n",
    "                                  size_sample  = size_sample_test,\n",
    "                                  size_sample_per_class = size_sample_per_class, \n",
    "                                  trans = trans, \n",
    "                                  seed  = None, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936ad14-bd33-48e4-9b4d-88e18dc7ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[[ Form a support set ]]]\n",
    "# Fetch all hit labels\n",
    "hit_list = list(set( [ hit for _, hit, _ in data_train ] ))\n",
    "\n",
    "# Form support set...\n",
    "support_hit_to_idx_dict = { hit : [] for hit in hit_list }\n",
    "for enum_data, (img, hit, metadata) in enumerate(data_train):\n",
    "    support_hit_to_idx_dict[hit].append(enum_data)\n",
    "\n",
    "for hit, idx_in_data_support in support_hit_to_idx_dict.items():\n",
    "    if len(support_hit_to_idx_dict[hit]) > num_max_support:\n",
    "        support_hit_to_idx_dict[hit] = random.sample(support_hit_to_idx_dict[hit], k = num_max_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62210b-c43e-4779-b901-74327ab63860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset...\n",
    "# Data preprocessing can be lengthy and defined in dataset_preprocess.py\n",
    "img_orig            = dataset_query[0][0][0]   # idx, fetch img\n",
    "dataset_preproc     = DatasetPreprocess(img_orig)\n",
    "trans               = dataset_preproc.config_trans()\n",
    "dataset_query.trans  = trans\n",
    "img_trans           = dataset_query[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfed5e-4877-4957-a3f0-b7989dd47365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[[ IMAGE ENCODER ]]]\n",
    "# Config the encoder...\n",
    "dim_emb        = 128\n",
    "size_y, size_x = img_trans.shape[-2:]\n",
    "config_encoder = ConfigEncoder( dim_emb = dim_emb,\n",
    "                                size_y  = size_y,\n",
    "                                size_x  = size_x,\n",
    "                                isbias  = True )\n",
    "encoder = Hirotaka0122(config_encoder)\n",
    "\n",
    "# Set up the model\n",
    "config_siamese = ConfigSiameseModel( encoder = encoder, )\n",
    "model = SiameseModelCompare(config_siamese)\n",
    "model.init_params(from_timestamp = timestamp)\n",
    "\n",
    "\n",
    "# Set up the right device for the computation...\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa068f-4a84-4b21-9c02-02550d5e583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[[ EMBEDDING (SUPPORT) ]]]\n",
    "support_batch_emb_dict = { hit : None for hit in hit_list }\n",
    "for hit in hit_list:\n",
    "    support_idx_list    = support_hit_to_idx_dict[hit]\n",
    "    num_example_support = len(support_idx_list)\n",
    "    for enum_support_idx, support_idx in enumerate(support_idx_list):\n",
    "        # Fetch data from support...\n",
    "        img = data_train[support_idx][0]\n",
    "        img = trans(img)\n",
    "        img = img[None,]\n",
    "\n",
    "        # Normalize the image...\n",
    "        img = (img - img.mean()) / img.std()\n",
    "\n",
    "        # Preallocate tensor...\n",
    "        if enum_support_idx == 0:\n",
    "            size_c, size_y, size_x = img.shape\n",
    "            batch_img = torch.zeros((num_example_support, size_c, size_y, size_x))\n",
    "\n",
    "        # Save image as tensor...\n",
    "        batch_img[enum_support_idx] = torch.tensor(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_img = batch_img.to(device = device)\n",
    "        support_batch_emb_dict[hit] = model.encoder.encode(batch_img)\n",
    "\n",
    "# [[[ EMBEDDING (QUERY) ]]]\n",
    "num_test       = len(dataset_query)\n",
    "query_idx_list = range(num_test)\n",
    "for enum_query_idx, i in enumerate(query_idx_list):\n",
    "    # Fetch data from query\n",
    "    img = dataset_query[i][0]\n",
    "\n",
    "    # Preallocate tensor...\n",
    "    if enum_query_idx == 0:\n",
    "        size_c, size_y, size_x = img.shape\n",
    "        batch_img = torch.zeros((num_test, size_c, size_y, size_x))\n",
    "\n",
    "    # Save image as tensor...\n",
    "    batch_img[enum_query_idx] = torch.tensor(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_img = batch_img.to(device = device)\n",
    "    query_batch_emb = model.encoder.encode(batch_img)\n",
    "\n",
    "\n",
    "# [[[ METRIC ]]] \n",
    "diff_query_support_dict = {}\n",
    "for hit in hit_list:\n",
    "    # Q: number of query examples.\n",
    "    # S: number of support examples\n",
    "    # E: dimension of an embedding\n",
    "    # diff_query_support_dict[hit]: Q x S x E \n",
    "    # query_batch_emb[:, None]    : Q x 1 x E \n",
    "    # support_batch_emb_dict[hit] :     S x E \n",
    "    diff_query_support_dict[hit] = query_batch_emb[:,None] - support_batch_emb_dict[hit]\n",
    "\n",
    "hit_to_dist_dict = {}\n",
    "for hit in hit_list:\n",
    "    # Q: number of query examples.\n",
    "    # S: number of support examples\n",
    "    # hit_to_dist_dict[hit]: Q x S \n",
    "    hit_to_dist_dict[hit] = torch.sum(diff_query_support_dict[hit] * diff_query_support_dict[hit], dim = -1) \n",
    "\n",
    "# Use enumeration as an intermediate to obtain predicted hits...\n",
    "enum_to_hit_dict = {}\n",
    "\n",
    "# Encode hit type with enum\n",
    "# enum 0 : hit 1\n",
    "# enum 1 : hit 2\n",
    "for enum_hit, hit in enumerate(hit_list):\n",
    "    enum_to_hit_dict[enum_hit] = hit \n",
    "\n",
    "    # Fetch the values and indices of the closet support for this hit type for the query...\n",
    "    mean_support_val = hit_to_dist_dict[hit].mean(dim = -1) \n",
    "    if enum_hit == 0:\n",
    "        # H: number of hit types (single vs multi)\n",
    "        # N: number of examples\n",
    "        # mean_support_tensor: H x N \n",
    "        mean_support_tensor = torch.zeros((len(hit_list), *mean_support_val.shape))\n",
    "\n",
    "    mean_support_tensor[enum_hit] = mean_support_val\n",
    "\n",
    "# Obtain the predicted hit...\n",
    "# Obtain the min among examples across all hit type (dim = 0)\n",
    "# [1] is to pick the indices from the result of a torch.min\n",
    "pred_hit_as_enum_list = mean_support_tensor.min(dim = 0)[1]\n",
    "\n",
    "# Obtain the predicted hit for each input example\n",
    "pred_hit_list = [ enum_to_hit_dict[enum.item()] for enum in pred_hit_as_enum_list ]\n",
    "\n",
    "# Obtain the real hit...\n",
    "real_hit_list = [ dataset_query[idx][1] for idx in query_idx_list ]\n",
    "\n",
    "# New container to store validation result (thus res_dict) for each label...\n",
    "res_dict = {}\n",
    "for hit in hit_list: res_dict[hit] = { i : [] for i in hit_list }\n",
    "\n",
    "for pred_hit, real_hit in zip(pred_hit_list, real_hit_list):\n",
    "    res_dict[pred_hit][real_hit].append( None )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f41a80-8e0b-4c42-91d9-ff6b358f225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get macro metrics...\n",
    "cm = ConfusionMatrix(res_dict)\n",
    "\n",
    "# Formating purpose...\n",
    "disp_dict = { 0 : \"not-sample\",\n",
    "              1 : \"single-hit\",\n",
    "              2 : \" multi-hit\",\n",
    "              9 : \"background\",\n",
    "            }\n",
    "\n",
    "# Report multiway classification...\n",
    "msgs = []\n",
    "for label_pred in sorted(hit_list):\n",
    "    disp_text = disp_dict[label_pred]\n",
    "    msg = f\"{disp_text}  |\"\n",
    "    for label_real in sorted(hit_list):\n",
    "        num = len(res_dict[label_pred][label_real])\n",
    "        msg += f\"{num:>12d}\"\n",
    "\n",
    "    metrics = cm.get_metrics(label_pred)\n",
    "    for metric in metrics:\n",
    "        msg += f\"{metric:>12.2f}\" if metric is not None else \"{x:>14s}\".format(x = \"None\")\n",
    "    msgs.append(msg)\n",
    "\n",
    "msg_header = \" \" * (msgs[0].find(\"|\") + 1)\n",
    "for label in sorted(hit_list): \n",
    "    disp_text = disp_dict[label]\n",
    "    msg_header += f\"{disp_text:>12s}\"\n",
    "\n",
    "for header in [ \"accuracy\", \"precision\", \"recall\", \"specificity\", \"f1\" ]:\n",
    "    msg_header += f\"{header:>12s}\"\n",
    "print(msg_header)\n",
    "\n",
    "msg_headerbar = \"-\" * len(msgs[0])\n",
    "print(msg_headerbar)\n",
    "for msg in msgs:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e297c-3627-415f-8c9f-edd44209515a",
   "metadata": {},
   "source": [
    "#### Equivalent confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae30c-6ad6-494b-a939-7a7ad2210b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_mod_list = (0, 1)\n",
    "res_mod_dict = {}\n",
    "for hit in {0, 1}: res_mod_dict[hit] = { i : [] for i in hit_mod_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5ec8a-7241-4e68-95b9-0caa739538c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_hit, real_hit in zip(pred_hit_list, real_hit_list):\n",
    "    pred_hit = 0 if pred_hit == 2 else pred_hit\n",
    "    real_hit = 0 if real_hit == 2 else real_hit\n",
    "    res_mod_dict[pred_hit][real_hit].append( None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc4037-a025-42bb-804e-39785dc73489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get macro metrics...\n",
    "cm = ConfusionMatrix(res_mod_dict)\n",
    "\n",
    "# Formating purpose...\n",
    "disp_dict = { 0 : \"not-sample\",\n",
    "              1 : \"single-hit\",\n",
    "              2 : \" multi-hit\",\n",
    "              9 : \"background\",\n",
    "            }\n",
    "\n",
    "# Report multiway classification...\n",
    "msgs = []\n",
    "for label_pred in sorted(hit_mod_list):\n",
    "    disp_text = disp_dict[label_pred]\n",
    "    msg = f\"{disp_text}  |\"\n",
    "    for label_real in sorted(hit_mod_list):\n",
    "        num = len(res_mod_dict[label_pred][label_real])\n",
    "        msg += f\"{num:>12d}\"\n",
    "\n",
    "    metrics = cm.get_metrics(label_pred)\n",
    "    for metric in metrics:\n",
    "        msg += f\"{metric:>12.2f}\" if metric is not None else \"{x:>14s}\".format(x = \"None\")\n",
    "    msgs.append(msg)\n",
    "\n",
    "msg_header = \" \" * (msgs[0].find(\"|\") + 1)\n",
    "for label in sorted(hit_mod_list): \n",
    "    disp_text = disp_dict[label]\n",
    "    msg_header += f\"{disp_text:>12s}\"\n",
    "\n",
    "for header in [ \"accuracy\", \"precision\", \"recall\", \"specificity\", \"f1\" ]:\n",
    "    msg_header += f\"{header:>12s}\"\n",
    "print(msg_header)\n",
    "\n",
    "msg_headerbar = \"-\" * len(msgs[0])\n",
    "print(msg_headerbar)\n",
    "for msg in msgs:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8ffc1-80e6-46a5-90c1-3b1f572a0cc6",
   "metadata": {},
   "source": [
    "#### Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16f28b-3a01-41de-acfc-18d8feb87225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29053f-21aa-4c6b-ba19-49f844ff8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    plt.figure(figsize = (5, 3))\n",
    "    #data = img_orig\n",
    "    data = dataset_query[0][0][0]\n",
    "    vmin = data.mean()\n",
    "    vmax = vmin + 1 * data.std()\n",
    "    plt.imshow(data, vmin = vmin, vmax = vmax)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac978e-56ec-4aa8-808f-b376ef7929dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana-ml-py3",
   "language": "python",
   "name": "ana-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

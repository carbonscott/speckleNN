{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35db868f-f302-4752-b303-62e6ec8ed9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import socket\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from deepprojection.datasets.lite    import SPIDataset, SPIOnlineDataset\n",
    "from deepprojection.model            import OnlineTripletSiameseModel, ConfigSiameseModel\n",
    "from deepprojection.validator        import MultiwayQueryValidator, ConfigValidator\n",
    "from deepprojection.encoders.convnet import FewShotModel, ConfigEncoder\n",
    "from deepprojection.utils            import EpochManager, MetaLog, init_logger, split_dataset, ConfusionMatrix, set_seed\n",
    "\n",
    "from image_preprocess_faulty_sq_for_validate import DatasetPreprocess\n",
    "\n",
    "# [[[ SEED ]]]\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "# [[[ CONFIG ]]]\n",
    "timestamp = \"2022_1219_1129_04\"\n",
    "epoch = 92\n",
    "\n",
    "fl_chkpt = f\"{timestamp}.epoch={epoch}.chkpt\"\n",
    "\n",
    "# Define the test set\n",
    "size_sample_query = 1000\n",
    "num_max_support   = 20\n",
    "frac_support      = 0.4\n",
    "size_batch        = 100\n",
    "trans             = None\n",
    "alpha = 0.02\n",
    "\n",
    "# Initialize a log file...\n",
    "init_logger(log_name = 'validate.query.test', timestamp = timestamp, returns_timestamp = False, saves_log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48989041-300d-4c0a-9b12-5d4579769584",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dat = 'skopi/pdb_num.dat'\n",
    "data = open(path_dat).readlines()\n",
    "pdb_size_dict = {}\n",
    "for d in data:\n",
    "    pdb, mol_size = d.strip().split()\n",
    "    pdb_size_dict[pdb] = int(mol_size)\n",
    "size_list = list(pdb_size_dict.values())\n",
    "size_nplist = np.array(size_list)\n",
    "hy, hx = np.histogram(size_nplist, bins = 50)\n",
    "size_pdb_dict = {}\n",
    "for enum_i, (size_min, size_max) in enumerate(zip(hx[:-1], hx[1:])):\n",
    "    size_pdb_dict[enum_i] = [ pdb for pdb, mol_size in pdb_size_dict.items() if size_min < mol_size < size_max ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414d3b96-cbe4-43a6-b627-14e1adf3ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d22ff-01c1-4ab4-9407-e400d7e08dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/19/2022 22:55:12 INFO image_preprocess_faulty_sq_for_validate - ___/ Preprocess Settings \\___\n",
      "12/19/2022 22:55:12 INFO image_preprocess_faulty_sq_for_validate - Apply Poisson noise. \n",
      "12/19/2022 22:55:12 INFO image_preprocess_faulty_sq_for_validate - Apply Gaussian noise. sigma = 0.15.\n",
      "12/19/2022 22:55:12 INFO image_preprocess_faulty_sq_for_validate - TRANS : Apply cropping.\n",
      "12/19/2022 22:55:12 INFO image_preprocess_faulty_sq_for_validate - TRANS : Apply random rotation. angle = None, center = (48, 48).\n",
      "12/19/2022 22:55:12 INFO deepprojection.encoders.convnet     - ___/ Configure Encoder \\___\n",
      "12/19/2022 22:55:12 INFO deepprojection.encoders.convnet     - KV - dim_emb          : 128\n",
      "12/19/2022 22:55:12 INFO deepprojection.encoders.convnet     - KV - size_y           : 96\n",
      "12/19/2022 22:55:12 INFO deepprojection.encoders.convnet     - KV - size_x           : 96\n",
      "12/19/2022 22:55:12 INFO deepprojection.encoders.convnet     - KV - isbias           : True\n",
      "12/19/2022 22:55:12 INFO deepprojection.model                - ___/ Configure Siamese Model \\___\n",
      "12/19/2022 22:55:12 INFO deepprojection.model                - KV - alpha            : 0.02\n",
      "12/19/2022 22:55:12 INFO deepprojection.model                - KV - encoder          : FewShotModel(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): PReLU(num_parameters=1)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (embed): Sequential(\n",
      "    (0): Linear(in_features=28224, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enum_pdb=000, pdb=6N38, acc=0.9750...\n",
      "enum_pdb=001, pdb=6QEM, acc=1.0000...\n",
      "enum_pdb=002, pdb=1VQM, acc=0.9980...\n",
      "enum_pdb=003, pdb=5U4J, acc=0.5970...\n",
      "enum_pdb=004, pdb=6YN1, acc=0.9830...\n",
      "enum_pdb=005, pdb=6W17, acc=0.9770...\n",
      "enum_pdb=006, pdb=6OR5, acc=0.3260...\n",
      "enum_pdb=007, pdb=6BWI, acc=0.9310...\n",
      "enum_pdb=008, pdb=6VBU, acc=0.9660...\n",
      "enum_pdb=009, pdb=6JP5, acc=0.9060...\n",
      "enum_pdb=010, pdb=7EW6, acc=0.9840...\n"
     ]
    }
   ],
   "source": [
    "pdb_candidate_list = size_pdb_dict[1]\n",
    "num_pdb_for_test = 100\n",
    "num_pdb_for_test = min(len(pdb_candidate_list), num_pdb_for_test)\n",
    "pdb_list = random.sample(pdb_candidate_list, k = num_pdb_for_test)\n",
    "acc_list = []\n",
    "for enum_pdb, pdb in enumerate(pdb_list):\n",
    "    # [[[ DATASET ]]]\n",
    "    # Set up parameters for an experiment...\n",
    "    drc_dataset  = 'fastdata.h5/'\n",
    "    fl_dataset   = f'{pdb}.relabel.pickle'\n",
    "    path_dataset = os.path.join(drc_dataset, fl_dataset)\n",
    "\n",
    "    # Load raw data...\n",
    "    with open(path_dataset, 'rb') as fh:\n",
    "        dataset_list = pickle.load(fh)\n",
    "\n",
    "    # Split data into two -- support set and query set...\n",
    "    data_support, data_query = split_dataset(dataset_list, frac_support, seed = None)\n",
    "    \n",
    "    # Fetch all hit labels...\n",
    "    hit_list = list(set( [ hit for _, (pdb, hit), _ in data_support ] ))\n",
    "\n",
    "    # Form support set...\n",
    "    support_hit_to_idx_dict = { hit : [] for hit in hit_list }\n",
    "    for enum_data, (img, label, metadata) in enumerate(data_support):\n",
    "        _, hit = label\n",
    "        support_hit_to_idx_dict[hit].append(enum_data)\n",
    "\n",
    "    for hit, idx_in_data_support in support_hit_to_idx_dict.items():\n",
    "        if len(support_hit_to_idx_dict[hit]) > num_max_support:\n",
    "            support_hit_to_idx_dict[hit] = random.sample(support_hit_to_idx_dict[hit], k = num_max_support)\n",
    "\n",
    "    # Form query dataset...\n",
    "    dataset_query = SPIOnlineDataset( dataset_list   = data_query, \n",
    "                                      size_sample    = size_sample_query,\n",
    "                                      joins_metadata = False,\n",
    "                                      trans          = trans, )\n",
    "\n",
    "    if enum_pdb == 0:\n",
    "        # [[[ Preprocess dataset ]]]\n",
    "        # Data preprocessing can be lengthy and defined in dataset_preprocess.py\n",
    "        img_orig            = dataset_query[0][0][0]   # idx, fetch img\n",
    "        dataset_preproc     = DatasetPreprocess(img_orig)\n",
    "        trans               = dataset_preproc.config_trans()\n",
    "        dataset_query.trans = trans\n",
    "        img_trans           = dataset_query[0][0][0]\n",
    "  \n",
    "        # [[[ IMAGE ENCODER ]]]\n",
    "        # Config the encoder...\n",
    "        dim_emb        = 128\n",
    "        size_y, size_x = img_trans.shape[-2:]\n",
    "        config_encoder = ConfigEncoder( dim_emb = dim_emb,\n",
    "                                        size_y  = size_y,\n",
    "                                        size_x  = size_x,\n",
    "                                        isbias  = True )\n",
    "        encoder = FewShotModel(config_encoder)\n",
    "        \n",
    "        # [[[ DEVICE ]]]\n",
    "        device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # [[[ MODEL ]]]\n",
    "        # Config the model...\n",
    "        config_siamese = ConfigSiameseModel( alpha = alpha, encoder = encoder, )\n",
    "        model = OnlineTripletSiameseModel(config_siamese)\n",
    "        model.init_params(fl_chkpt = fl_chkpt)\n",
    "        model.to(device = device)\n",
    "\n",
    "    # [[[ EMBEDDING (SUPPORT) ]]]\n",
    "    support_batch_emb_dict = { hit : None for hit in hit_list }\n",
    "    for hit in hit_list:\n",
    "        support_idx_list    = support_hit_to_idx_dict[hit]\n",
    "        num_example_support = len(support_idx_list)\n",
    "        for enum_support_idx, support_idx in enumerate(support_idx_list):\n",
    "            # Fetch data from support...\n",
    "            img = data_support[support_idx][0]\n",
    "            img = trans(img)\n",
    "            img = img[None,]\n",
    "\n",
    "            # Preallocate tensor...\n",
    "            if enum_support_idx == 0:\n",
    "                size_c, size_y, size_x = img.shape\n",
    "                batch_img = torch.zeros((num_example_support, size_c, size_y, size_x))\n",
    "\n",
    "            # Save image as tensor...\n",
    "            batch_img[enum_support_idx] = torch.tensor(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_img = batch_img.to(device = device)\n",
    "            support_batch_emb_dict[hit] = model.encoder.encode(batch_img)\n",
    "\n",
    "    # [[[ EMBEDDING (QUERY) ]]]\n",
    "    num_test       = len(dataset_query)\n",
    "    query_idx_list = range(num_test)\n",
    "    for enum_query_idx, i in enumerate(query_idx_list):\n",
    "        # Fetch data from query\n",
    "        img = dataset_query[i][0]\n",
    "\n",
    "        # Preallocate tensor...\n",
    "        if enum_query_idx == 0:\n",
    "            size_c, size_y, size_x = img.shape\n",
    "            batch_img = torch.zeros((num_test, size_c, size_y, size_x))\n",
    "\n",
    "        # Save image as tensor...\n",
    "        batch_img[enum_query_idx] = torch.tensor(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_img = batch_img.to(device = device)\n",
    "        query_batch_emb = model.encoder.encode(batch_img)\n",
    "\n",
    "\n",
    "    # [[[ METRIC ]]]\n",
    "    diff_query_support_hit_to_idx_dict = {}\n",
    "    for hit in hit_list:\n",
    "        diff_query_support_hit_to_idx_dict[hit] = query_batch_emb[:,None] - support_batch_emb_dict[hit]\n",
    "\n",
    "    dist_dict = {}\n",
    "    for hit in hit_list:\n",
    "        dist_dict[hit] = torch.sum(diff_query_support_hit_to_idx_dict[hit] * diff_query_support_hit_to_idx_dict[hit], dim = -1)\n",
    "\n",
    "    # Use Torch tensor as a facilitate to get the predicted hit...\n",
    "    enum_to_hit_dict = {}\n",
    "    for enum_hit, hit in enumerate(hit_list):\n",
    "        enum_to_hit_dict[enum_hit] = hit\n",
    "\n",
    "        # Fetch the values and indices the support with the min dist when measured against the query...\n",
    "        min_support_val, min_support_idx = dist_dict[hit].min(dim = -1)\n",
    "        if enum_hit == 0:\n",
    "            min_support_tensor = torch.zeros((len(hit_list), *min_support_val.shape))\n",
    "\n",
    "        min_support_tensor[enum_hit] = min_support_val\n",
    "\n",
    "    # Obtain the predicted hit...\n",
    "    pred_hit_as_enum_list = min_support_tensor.min(dim = 0)[1]\n",
    "    pred_hit_list = [ enum_to_hit_dict[enum.item()] for enum in pred_hit_as_enum_list ]\n",
    "\n",
    "    # Obtain the real hit...\n",
    "    real_hit_list = [ dataset_query[idx][1][1] for idx in query_idx_list ]\n",
    "\n",
    "    # New container to store validation result (thus res_dict) for each label...\n",
    "    res_dict = {}\n",
    "    for hit in hit_list: res_dict[hit] = { i : [] for i in hit_list }\n",
    "\n",
    "    for pred_hit, real_hit in zip(pred_hit_list, real_hit_list):\n",
    "        res_dict[pred_hit][real_hit].append( None )\n",
    "    \n",
    "    confusion_matrix = ConfusionMatrix(res_dict)\n",
    "    acc = confusion_matrix.get_metrics(1)[0]\n",
    "    acc_list.append((pdb, acc))\n",
    "\n",
    "    print(f\"enum_pdb={enum_pdb:03d}, pdb={pdb}, acc={acc:.4f}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53dc19b-2269-4774-a122-7397613a8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "hittype_pred, hittype_real, test_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f61647-ab36-4d09-b125-a80b194510e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8320532-392d-4a01-abe2-fde4ff719fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_np = np.array([ acc[1] for acc in acc_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c046b0d-8af7-404b-9167-9dda6b2f192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_np.mean(), acc_np.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edde73a-fbba-4a8f-b46e-76201f5bec9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74835d38-45f7-436c-9f4b-88ea38ef9bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b144fc-ca4d-46a5-a1c9-a1eebb451e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pdb_for_test = 100\n",
    "pdb_list = random.sample(size_pdb_dict[0], k = num_pdb_for_test)\n",
    "pdb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45a43f-0626-497c-8a85-452f5d191c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana-ml-py3",
   "language": "python",
   "name": "ana-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

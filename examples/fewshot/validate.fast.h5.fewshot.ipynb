{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3deead04-6a61-4c14-ab55-80ec240a9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/19/2022 21:48:00 INFO image_preprocess_faulty_sq_for_validate - ___/ Preprocess Settings \\___\n",
      "12/19/2022 21:48:00 INFO image_preprocess_faulty_sq_for_validate - Apply Poisson noise. \n",
      "12/19/2022 21:48:00 INFO image_preprocess_faulty_sq_for_validate - Apply Gaussian noise. sigma = 0.15.\n",
      "12/19/2022 21:48:00 INFO image_preprocess_faulty_sq_for_validate - TRANS : Apply cropping.\n",
      "12/19/2022 21:48:00 INFO image_preprocess_faulty_sq_for_validate - TRANS : Apply random rotation. angle = None, center = (48, 48).\n",
      "12/19/2022 21:48:00 INFO deepprojection.encoders.convnet     - ___/ Configure Encoder \\___\n",
      "12/19/2022 21:48:00 INFO deepprojection.encoders.convnet     - KV - dim_emb          : 128\n",
      "12/19/2022 21:48:00 INFO deepprojection.encoders.convnet     - KV - size_y           : 96\n",
      "12/19/2022 21:48:00 INFO deepprojection.encoders.convnet     - KV - size_x           : 96\n",
      "12/19/2022 21:48:00 INFO deepprojection.encoders.convnet     - KV - isbias           : True\n",
      "12/19/2022 21:48:00 INFO deepprojection.model                - ___/ Configure Siamese Model \\___\n",
      "12/19/2022 21:48:00 INFO deepprojection.model                - KV - alpha            : 0.02\n",
      "12/19/2022 21:48:00 INFO deepprojection.model                - KV - encoder          : FewShotModel(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): PReLU(num_parameters=1)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (embed): Sequential(\n",
      "    (0): Linear(in_features=28224, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import socket\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from deepprojection.datasets.lite    import SPIDataset, SPIOnlineDataset\n",
    "from deepprojection.model            import OnlineTripletSiameseModel, ConfigSiameseModel\n",
    "from deepprojection.validator        import MultiwayQueryValidator, ConfigValidator\n",
    "from deepprojection.encoders.convnet import FewShotModel, ConfigEncoder, FewShotModel2\n",
    "from deepprojection.utils            import EpochManager, MetaLog, init_logger, split_dataset, ConfusionMatrix, set_seed\n",
    "\n",
    "from image_preprocess_faulty_sq_for_validate import DatasetPreprocess\n",
    "\n",
    "# [[[ SEED ]]]\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "# [[[ CONFIG ]]]\n",
    "# Set the timestamp and epoch to load the corresponding model\n",
    "timestamp = \"2022_1219_1129_04\"\n",
    "epoch = 92\n",
    "\n",
    "# Define the pdb...\n",
    "pdb = '6QEM'\n",
    "\n",
    "# Define the chkpt file...\n",
    "fl_chkpt = f\"{timestamp}.epoch={epoch}.chkpt\"\n",
    "\n",
    "# Define the test set\n",
    "size_sample_query = 1000\n",
    "num_max_support   = 10\n",
    "frac_support      = 0.4\n",
    "size_batch        = 100\n",
    "trans             = None\n",
    "alpha             = 0.02\n",
    "\n",
    "# Initialize a log file...\n",
    "init_logger(log_name = 'validate.query.test', timestamp = timestamp, returns_timestamp = False, saves_log = False)\n",
    "\n",
    "\n",
    "# [[[ DATASET ]]]\n",
    "# Set up parameters for an experiment...\n",
    "drc_dataset  = 'fastdata.h5/'\n",
    "fl_dataset   = f'{pdb}.relabel.pickle'\n",
    "path_dataset = os.path.join(drc_dataset, fl_dataset)\n",
    "\n",
    "# Load raw data...\n",
    "with open(path_dataset, 'rb') as fh:\n",
    "    dataset_list = pickle.load(fh)\n",
    "\n",
    "# Split data into two -- support set and query set...\n",
    "data_support, data_query = split_dataset(dataset_list, frac_support, seed = None)\n",
    "\n",
    "# Fetch all hit labels...\n",
    "hit_list = list(set( [ hit for _, (pdb, hit), _ in data_support ] ))\n",
    "\n",
    "# Form support set...\n",
    "support_hit_to_idx_dict = { hit : [] for hit in hit_list }\n",
    "for enum_data, (img, label, metadata) in enumerate(data_support):\n",
    "    _, hit = label\n",
    "    support_hit_to_idx_dict[hit].append(enum_data)\n",
    "\n",
    "for hit, idx_in_data_support in support_hit_to_idx_dict.items():\n",
    "    if len(support_hit_to_idx_dict[hit]) > num_max_support:\n",
    "        support_hit_to_idx_dict[hit] = random.sample(support_hit_to_idx_dict[hit], k = num_max_support)\n",
    "\n",
    "# Form query dataset...\n",
    "dataset_query = SPIOnlineDataset( dataset_list   = data_query, \n",
    "                                  size_sample    = size_sample_query,\n",
    "                                  joins_metadata = False,\n",
    "                                  trans          = trans, )\n",
    "\n",
    "# Preprocess dataset...\n",
    "# Data preprocessing can be lengthy and defined in dataset_preprocess.py\n",
    "img_orig            = dataset_query[0][0][0]   # idx, fetch img\n",
    "dataset_preproc     = DatasetPreprocess(img_orig)\n",
    "trans               = dataset_preproc.config_trans()\n",
    "dataset_query.trans = trans\n",
    "img_trans           = dataset_query[0][0][0]\n",
    "\n",
    "\n",
    "# [[[ IMAGE ENCODER ]]]\n",
    "# Config the encoder...\n",
    "dim_emb        = 128\n",
    "size_y, size_x = img_trans.shape[-2:]\n",
    "config_encoder = ConfigEncoder( dim_emb = dim_emb,\n",
    "                                size_y  = size_y,\n",
    "                                size_x  = size_x,\n",
    "                                isbias  = True )\n",
    "encoder = FewShotModel(config_encoder)\n",
    "# encoder = FewShotModel2(config_encoder)\n",
    "\n",
    "# [[[ DEVICE ]]]\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# [[[ MODEL ]]]\n",
    "# Config the model...\n",
    "config_siamese = ConfigSiameseModel( alpha = alpha, encoder = encoder, )\n",
    "model = OnlineTripletSiameseModel(config_siamese)\n",
    "model.init_params(fl_chkpt = fl_chkpt)\n",
    "model.to(device = device)\n",
    "\n",
    "# [[[ EMBEDDING (SUPPORT) ]]]\n",
    "support_batch_emb_dict = { hit : None for hit in hit_list }\n",
    "for hit in hit_list:\n",
    "    support_idx_list    = support_hit_to_idx_dict[hit]\n",
    "    num_example_support = len(support_idx_list)\n",
    "    for enum_support_idx, support_idx in enumerate(support_idx_list):\n",
    "        # Fetch data from support...\n",
    "        img = data_support[support_idx][0]\n",
    "        img = trans(img)\n",
    "        img = img[None,]\n",
    "\n",
    "        # Preallocate tensor...\n",
    "        if enum_support_idx == 0:\n",
    "            size_c, size_y, size_x = img.shape\n",
    "            batch_img = torch.zeros((num_example_support, size_c, size_y, size_x))\n",
    "\n",
    "        # Save image as tensor...\n",
    "        batch_img[enum_support_idx] = torch.tensor(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_img = batch_img.to(device = device)\n",
    "        support_batch_emb_dict[hit] = model.encoder.encode(batch_img)\n",
    "\n",
    "\n",
    "# [[[ EMBEDDING (QUERY) ]]]\n",
    "num_test       = len(dataset_query)\n",
    "query_idx_list = range(num_test)\n",
    "for enum_query_idx, i in enumerate(query_idx_list):\n",
    "    # Fetch data from query\n",
    "    img = dataset_query[i][0]\n",
    "\n",
    "    # Preallocate tensor...\n",
    "    if enum_query_idx == 0:\n",
    "        size_c, size_y, size_x = img.shape\n",
    "        batch_img = torch.zeros((num_test, size_c, size_y, size_x))\n",
    "\n",
    "    # Save image as tensor...\n",
    "    batch_img[enum_query_idx] = torch.tensor(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_img = batch_img.to(device = device)\n",
    "    query_batch_emb = model.encoder.encode(batch_img)\n",
    "\n",
    "\n",
    "# [[[ METRIC ]]]\n",
    "diff_query_support_hit_to_idx_dict = {}\n",
    "for hit in hit_list:\n",
    "    diff_query_support_hit_to_idx_dict[hit] = query_batch_emb[:,None] - support_batch_emb_dict[hit]\n",
    "\n",
    "dist_dict = {}\n",
    "for hit in hit_list:\n",
    "    dist_dict[hit] = torch.sum(diff_query_support_hit_to_idx_dict[hit] * diff_query_support_hit_to_idx_dict[hit], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbc9920-09f9-43c6-bee3-fc94e7613674",
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_to_hit_dict = {}\n",
    "for enum_hit, hit in enumerate(hit_list):\n",
    "    enum_to_hit_dict[enum_hit] = hit\n",
    "    \n",
    "    # Fetch the values and indices the support with the min dist when measured against the query...\n",
    "    min_support_val, min_support_idx = dist_dict[hit].min(dim = -1)\n",
    "    if enum_hit == 0:\n",
    "        min_support_tensor = torch.zeros((len(hit_list), *min_support_val.shape))\n",
    "        \n",
    "    min_support_tensor[enum_hit] = min_support_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e399fa9d-6c69-4dfd-8ab6-14abf60f3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hit_as_enum_list = min_support_tensor.min(dim = 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ed7d95-ad53-4719-b6ad-0479afd68894",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hit_list = [ enum_to_hit_dict[enum.item()] for enum in pred_hit_as_enum_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1a07e1-d1da-410c-a276-693512213ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_hit_list = [ dataset_query[idx][1][1] for idx in query_idx_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d47c60-d713-48e3-884a-48bba8b985bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New container to store validation result (thus res_dict) for each label...\n",
    "res_dict = {}\n",
    "for hit in hit_list: res_dict[hit] = { i : [] for i in hit_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf1618e5-be33-4b34-a35c-403c5b0a2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_hit, real_hit in zip(pred_hit_list, real_hit_list):\n",
    "    res_dict[pred_hit][real_hit].append( None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66300e03-f7b2-4668-8645-44fb3dba4743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               single-hit   multi-hit    accuracy   precision      recall specificity          f1\n",
      "-------------------------------------------------------------------------------------------------\n",
      "single-hit  |         503           0        1.00        1.00        1.00        1.00        1.00\n",
      " multi-hit  |           0         497        1.00        1.00        1.00        1.00        1.00\n"
     ]
    }
   ],
   "source": [
    "# Get macro metrics...\n",
    "confusion_matrix = ConfusionMatrix(res_dict)\n",
    "\n",
    "# Formating purpose...\n",
    "disp_dict = {\n",
    "              1 : \"single-hit\",\n",
    "              2 : \" multi-hit\",\n",
    "            }\n",
    "\n",
    "# Report multiway classification...\n",
    "msgs = []\n",
    "for pred_hit in sorted(hit_list):\n",
    "    disp_text = disp_dict[pred_hit]\n",
    "    msg = f\"{disp_text}  |\"\n",
    "    for real_hit in sorted(hit_list):\n",
    "        num = len(res_dict[pred_hit][real_hit])\n",
    "        msg += f\"{num:>12d}\"\n",
    "\n",
    "    metrics = confusion_matrix.get_metrics(pred_hit)\n",
    "    for metric in metrics:\n",
    "        msg += f\"{metric:>12.2f}\"\n",
    "    msgs.append(msg)\n",
    "\n",
    "msg_header = \" \" * (msgs[0].find(\"|\") + 1)\n",
    "for label in sorted(hit_list): \n",
    "    disp_text = disp_dict[label]\n",
    "    msg_header += f\"{disp_text:>12s}\"\n",
    "\n",
    "for header in [ \"accuracy\", \"precision\", \"recall\", \"specificity\", \"f1\" ]:\n",
    "    msg_header += f\"{header:>12s}\"\n",
    "print(msg_header)\n",
    "\n",
    "msg_headerbar = \"-\" * len(msgs[0])\n",
    "print(msg_headerbar)\n",
    "for msg in msgs:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03c9be-8c55-425f-aec1-a6445dd65b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.get_metrics(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9cfed-f28d-49a0-928c-14c18a024388",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize query and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13fa61-af8c-450c-9b63-d51353b51556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d4ecf-2a07-4a80-9033-0480b2616fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_test = len(dataset_query)\n",
    "query_idx_list = range(num_test)\n",
    "\n",
    "min_val_idx_dict = {}\n",
    "for hit in (1, 2):\n",
    "    min_val_idx_dict[hit] = dist_dict[hit].min(dim = -1)\n",
    "    \n",
    "global_query_support_list = []\n",
    "msg_list = []\n",
    "for idx_enum, idx_query in enumerate(query_idx_list):\n",
    "    query_support_list = [idx_query]\n",
    "    val_support_list = []\n",
    "    for hit in (1, 2):\n",
    "        idx_best_support = min_val_idx_dict[hit][1][idx_query]\n",
    "        idx_best_support = support_hit_to_idx_dict[hit][idx_best_support.item()]\n",
    "        query_support_list.append(idx_best_support)\n",
    "        \n",
    "        val_best_support = min_val_idx_dict[hit][0][idx_query]\n",
    "        val_support_list.append(val_best_support)\n",
    "    global_query_support_list.append(query_support_list)\n",
    "    \n",
    "    metadata_query = dataset_query[idx_query][2]\n",
    "    metadata_support = ' '.join([ data_support[query_support_list[hit]][2] for hit in (1, 2) ])    # Spaghetti code, hit happens to be 1 and 2\n",
    "    label_pred = '1' if val_support_list[0] < val_support_list[1] else '2'\n",
    "    msg_list.append(f\"{metadata_query}, {metadata_support}, label = {label_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbba52-06f1-4612-a395-17b1a366175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_query.trans = trans\n",
    "dataset_query.trans = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dda2a-3d16-45ba-b9f2-2de6eca4f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_query = 126\n",
    "print(msg_list[idx_query])\n",
    "\n",
    "q, s1, s2 = global_query_support_list[idx_query]\n",
    "\n",
    "img, label, metadata = dataset_query[q]\n",
    "plt.figure(figsize = (10, 8))\n",
    "img = img[0]\n",
    "vmin = img.mean() - 0.5 * img.std()\n",
    "vmax = img.mean() + 4.0* img.std()\n",
    "plt.imshow(img, vmin = vmin, vmax = vmax)\n",
    "plt.suptitle(metadata)\n",
    "plt.colorbar()\n",
    "\n",
    "for s in (s1, s2):\n",
    "    img, label, metadata = data_support[s]\n",
    "    if dataset_query.trans is not None: img = dataset_query.trans(img)\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    vmin = img.mean() - 0.5 * img.std()\n",
    "    vmax = img.mean() + 4.0* img.std()\n",
    "    plt.imshow(img, vmin = vmin, vmax = vmax)\n",
    "    plt.suptitle(metadata)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccf4f0-bfec-48bb-9d24-bdf57a82d50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana-ml-py3",
   "language": "python",
   "name": "ana-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
